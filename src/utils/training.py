"""
Training utilities for GPT model
Connects tokenizer + dataset + model + optimizer for complete training pipeline
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import math
import sys
import os
from typing import Dict, Tuple, Optional
from tqdm import tqdm

# Add root directory to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

def calculate_loss(logits: torch.Tensor, targets: torch.Tensor, 
                   ignore_index: int = -100) -> torch.Tensor:
    """Calculate cross-entropy loss for next token prediction"""
    batch_size, seq_len, vocab_size = logits.shape
    
    logits_flat = logits.view(-1, vocab_size)
    targets_flat = targets.view(-1)
    
    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=ignore_index)
    return loss

def calculate_perplexity(loss: torch.Tensor) -> float:
    """Calculate perplexity from loss"""
    return math.exp(loss.item())

# ... [Keep all the Trainer class code the same] ...

class Trainer:
    """Complete training pipeline for GPT model"""
    
    def __init__(self, model: nn.Module, optimizer: torch.optim.Optimizer, 
                 device: str = "cpu"):
        self.model = model
        self.optimizer = optimizer
        self.device = device
        
        self.model.to(device)
        
        self.step_count = 0
        self.epoch_count = 0
        self.train_losses = []
        self.val_losses = []
        
        print(f"‚úÖ Trainer initialized on {device}")
        print(f"üìä Model parameters: {self._count_parameters():,}")
    
    def _count_parameters(self) -> int:
        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)
    
    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        input_ids = batch['input_ids'].to(self.device)
        target_ids = batch['target_ids'].to(self.device)
        
        self.model.train()
        logits = self.model(input_ids)
        
        loss = calculate_loss(logits, target_ids)
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        self.step_count += 1
        perplexity = calculate_perplexity(loss)
        
        return {
            'loss': loss.item(),
            'perplexity': perplexity,
            'step': self.step_count
        }
    
    def validate(self, dataloader: DataLoader) -> Dict[str, float]:
        self.model.eval()
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch['input_ids'].to(self.device)
                target_ids = batch['target_ids'].to(self.device)
                
                logits = self.model(input_ids)
                loss = calculate_loss(logits, target_ids)
                
                batch_size, seq_len = input_ids.shape
                total_loss += loss.item() * (batch_size * seq_len)
                total_tokens += batch_size * seq_len
        
        avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
        avg_perplexity = calculate_perplexity(torch.tensor(avg_loss))
        
        return {
            'val_loss': avg_loss,
            'val_perplexity': avg_perplexity
        }
    
    def train_epoch(self, dataloader: DataLoader, log_interval: int = 10) -> Dict[str, float]:
        epoch_losses = []
        
        pbar = tqdm(dataloader, desc=f"Epoch {self.epoch_count + 1}")
        
        for batch_idx, batch in enumerate(pbar):
            metrics = self.train_step(batch)
            epoch_losses.append(metrics['loss'])
            
            if batch_idx % log_interval == 0:
                pbar.set_postfix({
                    'loss': f"{metrics['loss']:.4f}",
                    'ppl': f"{metrics['perplexity']:.2f}"
                })
        
        self.epoch_count += 1
        
        avg_loss = sum(epoch_losses) / len(epoch_losses)
        avg_perplexity = calculate_perplexity(torch.tensor(avg_loss))
        
        epoch_metrics = {
            'epoch': self.epoch_count,
            'train_loss': avg_loss,
            'train_perplexity': avg_perplexity
        }
        
        self.train_losses.append(avg_loss)
        return epoch_metrics

def create_training_pipeline(config):
    """Create complete training pipeline from config"""
    from src.data.tokenizer import GPTTokenizer
    from src.data.dataset import TextDataset
    from src.model.transformer import GPTModel
    
    print("üöÄ Creating Training Pipeline...")
    
    tokenizer = GPTTokenizer()
    
    # Robust dataset creation with fallback
    try:
        if (hasattr(config.data, 'train_data_path') and 
            config.data.train_data_path and 
            os.path.exists(config.data.train_data_path) and 
            os.path.getsize(config.data.train_data_path) > 0):
            
            dataset = TextDataset.from_file(
                config.data.train_data_path, 
                tokenizer, 
                config.model.block_size
            )
        else:
            raise FileNotFoundError("No valid training data found")
            
    except Exception as e:
        print(f"‚ö†Ô∏è  {e}. Using built-in sample text.")
        # Create substantial sample text
        sample_text = """
        The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet.
        Machine learning is revolutionizing how we process and understand data. Neural networks learn complex patterns.
        Transformers use attention mechanisms to focus on relevant parts of input sequences.
        Language models predict the next word in a sequence by learning from vast amounts of text data.
        Training requires substantial computational resources and careful hyperparameter tuning.
        The attention mechanism allows models to weigh the importance of different input tokens.
        Gradient descent optimizes model parameters by minimizing prediction errors.
        Backpropagation computes gradients efficiently through the computational graph.
        Deep learning has enabled breakthroughs in natural language processing and computer vision.
        """ * 20  # Repeat to ensure sufficient tokens
        
        dataset = TextDataset(sample_text, tokenizer, config.model.block_size)
    
    dataloader = DataLoader(
        dataset, 
        batch_size=config.training.batch_size,
        shuffle=True,
        num_workers=0,  # Set to 0 for compatibility
        pin_memory=False  # Set to False for CPU
    )
    
    model = GPTModel(
        vocab_size=config.model.vocab_size,
        d_model=config.model.d_model,
        n_layers=config.model.n_layers,
        n_heads=config.model.n_heads,
        d_ff=config.model.d_ff,
        max_seq_len=config.model.block_size,
        dropout=config.model.dropout
    )
    
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.training.learning_rate,
        weight_decay=config.training.weight_decay,
        betas=(config.training.beta1, config.training.beta2)
    )
    
    trainer = Trainer(model, optimizer, config.training.device)
    
    print("‚úÖ Training pipeline ready!")
    return model, trainer, dataloader, tokenizer

def test_training_step():
    """Test single training step"""
    print("üß™ Testing Single Training Step...")
    
    # Import config here to avoid path issues
    from config import get_small_config
    config = get_small_config()
    
    model, trainer, dataloader, tokenizer = create_training_pipeline(config)
    
    batch = next(iter(dataloader))
    
    print(f"üìä Batch shapes:")
    print(f"   Input IDs: {batch['input_ids'].shape}")
    print(f"   Target IDs: {batch['target_ids'].shape}")
    print(f"   Sample input: {batch['input_ids'][0][:5].tolist()}")
    print(f"   Sample target: {batch['target_ids'][0][:5].tolist()}")
    
    initial_loss = None
    for step in range(3):
        metrics = trainer.train_step(batch)
        
        if step == 0:
            initial_loss = metrics['loss']
        
        print(f"Step {step + 1}: Loss = {metrics['loss']:.4f}, Perplexity = {metrics['perplexity']:.2f}")
    
    final_loss = metrics['loss']
    loss_decreased = final_loss < initial_loss
    
    print(f"‚úÖ Loss decreased: {loss_decreased} ({initial_loss:.4f} ‚Üí {final_loss:.4f})")
    print("‚úÖ Training step test PASSED!")
    return True

def test_full_pipeline():
    """Test complete training pipeline"""
    print("\nüß™ Testing Full Training Pipeline...")
    
    from config import get_small_config
    config = get_small_config()
    
    config.training.max_epochs = 2
    config.training.batch_size = 2
    
    model, trainer, dataloader, tokenizer = create_training_pipeline(config)
    
    print(f"üìä Pipeline info:")
    print(f"   Dataset size: {len(dataloader.dataset)} sequences")
    print(f"   Batches per epoch: {len(dataloader)}")
    print(f"   Model parameters: {trainer._count_parameters():,}")
    
    print(f"\nüöÄ Training for 1 epoch...")
    epoch_metrics = trainer.train_epoch(dataloader, log_interval=max(1, len(dataloader)//2))
    
    print(f"üìä Epoch results:")
    print(f"   Train loss: {epoch_metrics['train_loss']:.4f}")
    print(f"   Train perplexity: {epoch_metrics['train_perplexity']:.2f}")
    
    print("‚úÖ Full pipeline test PASSED!")
    return True

def test_text_generation():
    """Test basic text generation"""
    print("\nüß™ Testing Text Generation...")
    
    from config import get_small_config
    config = get_small_config()
    
    model, trainer, dataloader, tokenizer = create_training_pipeline(config)
    
    def generate_text(model, tokenizer, prompt: str, max_length: int = 15):
        """Generate text from prompt"""
        model.eval()
        
        token_ids = tokenizer.encode(prompt)
        context = torch.tensor([token_ids])
        
        print(f"Prompt: '{prompt}'")
        print(f"Prompt tokens: {token_ids[:5]}")
        
        generated_tokens = token_ids.copy()
        
        with torch.no_grad():
            for _ in range(max_length):
                logits = model(context)
                
                next_token_logits = logits[0, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1).item()
                
                generated_tokens.append(next_token)
                context = torch.cat([context, torch.tensor([[next_token]])], dim=1)
                
                if context.shape[1] >= config.model.block_size:
                    break
        
        generated_text = tokenizer.decode(generated_tokens)
        return generated_text, generated_tokens
    
    prompt = "The"
    generated_text, generated_tokens = generate_text(model, tokenizer, prompt, max_length=10)
    
    print(f"Generated text: '{generated_text}'")
    print(f"Generated tokens: {generated_tokens[:15]}")
    print(f"Generation length: {len(generated_tokens)} tokens")
    
    print("‚úÖ Text generation test PASSED!")
    return True

if __name__ == "__main__":
    print("üöÄ Testing Training Integration")
    print("=" * 70)
    
    tests = [
        test_training_step,
        test_full_pipeline,
        test_text_generation
    ]
    
    results = []
    for test_func in tests:
        try:
            result = test_func()
            results.append(result)
        except Exception as e:
            print(f"‚ùå {test_func.__name__} FAILED: {e}")
            import traceback
            traceback.print_exc()
            results.append(False)
    
    print("\n" + "=" * 70)
    passed = sum(results)
    total = len(results)
    
    if passed == total:
        print("üéâ ALL TRAINING INTEGRATION TESTS PASSED!")
        print("‚úÖ Step 2.2 COMPLETE! Ready for Step 2.3: Full Training!")
    else:
        print(f"‚ùå {total - passed}/{total} tests failed. Please fix before proceeding.")
